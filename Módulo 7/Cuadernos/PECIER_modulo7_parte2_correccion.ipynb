{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa922b7",
      "metadata": {
        "id": "1aa922b7"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"API-Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83dfb292",
      "metadata": {
        "id": "83dfb292"
      },
      "source": [
        "# Uso de APIs para interactuar con Modelos de Lenguaje (LLMs) como GPT\n",
        "\n",
        "En esta sección se estudiará la arquitectura básica de los LLMs y sus APIs, para pprender a configurar y autenticarse con APIs de modelos de lenguaje. Adicionalmente se especificará el envío de prompts y procesamiento de respuestas, para implementar casos de uso avanzados en el sector eléctrico."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154dac71",
      "metadata": {
        "id": "154dac71"
      },
      "source": [
        "## Introducción a los LLMs y sus APIs\n",
        "\n",
        "Los Modelos de Lenguaje (LLMs) son sistemas de IA entrenados para comprender y generar texto similar al humano. GPT (Generative Pre-trained Transformer) es uno de los modelos más avanzados, el cual se produjo después de una serie de modelos desarrollados para predecir la siguiente palabra en una secuencia. Con este principio se desarrollan sistemas de chat, de traducción, de clasificación de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa736fb",
      "metadata": {
        "id": "2aa736fb"
      },
      "source": [
        "**¿Por qué usar APIs?**\n",
        "- Además del acceso a fuentes de datos, las API sin pueden utilizar para enviar información (solicitudes, prompts) a LLMs, como ChatGPT y obtener la respuesta. Esto permite integración con otras aplicaciones y el desarrollo de programas escalables y de mayor funcionalidad.\n",
        "\n",
        "Dado que los modelos de lenguaje son desarrollados por empresas que cobran por utilizarlos (aunque hay gratuitos),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c3ea01",
      "metadata": {
        "id": "51c3ea01"
      },
      "outputs": [],
      "source": [
        "# Ejemplo básico de conexión a la API de OpenAI\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"tu-api-key\"  # Nunca expongas esto en código público\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explica cómo funciona un transformador eléctrico\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca765dd8",
      "metadata": {
        "id": "ca765dd8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Nota: Ya están cargadas previamente el openai y el api-key\n",
        "# Ejemplo básico de conexión a la API de OpenAI\n",
        "\n",
        "#import openai\n",
        "\n",
        "#openai.api_key = \"tu-api-key\"  # Nunca expongas esto en código público\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explica cómo funciona un transformador eléctrico\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4682411c",
      "metadata": {
        "id": "4682411c"
      },
      "source": [
        "\n",
        "## Interacción Básica: Prompts y Respuestas\n",
        "\n",
        "### Estructura de un Prompt Efectivo\n",
        "1. **Contexto**: Establece el rol y conocimiento necesario\n",
        "2. **Instrucción**: Qué debe hacer el modelo\n",
        "3. **Ejemplos** (opcional): Demostraciones del formato esperado\n",
        "4. **Pregunta/Consulta**: Solicitud específica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da22df43",
      "metadata": {
        "id": "da22df43"
      },
      "source": [
        "```python\n",
        "prompt_tecnico = \"\"\"\n",
        "Eres un ingeniero eléctrico senior con 20 años de experiencia en distribución de energía.\n",
        "Explica en términos técnicos pero accesibles el concepto de flujo de carga en redes eléctricas,\n",
        "incluyendo:\n",
        "\n",
        "1. Definición técnica\n",
        "2. Importancia en la operación del sistema\n",
        "3. Ejemplo numérico simple\n",
        "4. Consideraciones prácticas\n",
        "\n",
        "Usa ecuaciones cuando sea relevante y formato Markdown para la respuesta.\n",
        "\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "  model=\"gpt-4\",\n",
        "  messages=[{\"role\": \"user\", \"content\": prompt_tecnico}],\n",
        "  temperature=0.7  # Controla la creatividad (0-2)\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501bdb71",
      "metadata": {
        "id": "501bdb71"
      },
      "outputs": [],
      "source": [
        "prompt_tecnico = \"\"\"\n",
        "Eres un ingeniero eléctrico senior con 20 años de experiencia en distribución de energía.\n",
        "Explica en términos técnicos pero accesibles el concepto de flujo de carga en redes eléctricas,\n",
        "incluyendo:\n",
        "\n",
        "1. Definición técnica\n",
        "2. Importancia en la operación del sistema\n",
        "3. Ejemplo numérico simple\n",
        "4. Consideraciones prácticas\n",
        "\n",
        "Usa ecuaciones cuando sea relevante y formato Markdown para la respuesta.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_tecnico}],\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "599f5d0c",
      "metadata": {
        "id": "599f5d0c"
      },
      "source": [
        "## Casos de Uso\n",
        "\n",
        "### 1. Generación de Reportes Técnicos\n",
        "```python\n",
        "# Datos de entrada\n",
        "datos_subestacion = {\n",
        "    \"nombre\": \"Subestación Norte\",\n",
        "    \"capacidad\": \"150 MVA\",\n",
        "    \"nivel_voltaje\": \"230/115 kV\",\n",
        "    \"incidentes\": [\"Sobretensión 15/11\", \"Mantenimiento programado 20/11\"]\n",
        "}\n",
        "\n",
        "# Prompt\n",
        "prompt_reporte = f\"\"\"\n",
        "Genera un reporte ejecutivo de una página sobre el estado de la {datos_subestacion['nombre']} con:\n",
        "- Capacidad: {datos_subestacion['capacidad']}\n",
        "- Nivel de voltaje: {datos_subestacion['nivel_voltaje']}\n",
        "\n",
        "Incluye:\n",
        "1. Resumen operativo\n",
        "2. Análisis de incidentes recientes: {', '.join(datos_subestacion['incidentes'])}\n",
        "3. Recomendaciones de mantenimiento\n",
        "4. Riesgos potenciales\n",
        "\n",
        "Formato: Encabezado, secciones claras, puntos clave destacados.\n",
        "\"\"\"\n",
        "\n",
        "# Llamada a la API\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_reporte}],\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Mostrar resultado\n",
        "print(response.choices[0].message.content)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800921f8",
      "metadata": {
        "id": "800921f8"
      },
      "outputs": [],
      "source": [
        "# Datos de entrada\n",
        "datos_subestacion = {\n",
        "    \"nombre\": \"Subestación Norte\",\n",
        "    \"capacidad\": \"150 MVA\",\n",
        "    \"nivel_voltaje\": \"230/115 kV\",\n",
        "    \"incidentes\": [\"Sobretensión 15/11\", \"Mantenimiento programado 20/11\"]\n",
        "}\n",
        "\n",
        "# Prompt\n",
        "prompt_reporte = f\"\"\"\n",
        "Genera un reporte ejecutivo de una página sobre el estado de la {datos_subestacion['nombre']} con:\n",
        "- Capacidad: {datos_subestacion['capacidad']}\n",
        "- Nivel de voltaje: {datos_subestacion['nivel_voltaje']}\n",
        "\n",
        "Incluye:\n",
        "1. Resumen operativo\n",
        "2. Análisis de incidentes recientes: {', '.join(datos_subestacion['incidentes'])}\n",
        "3. Recomendaciones de mantenimiento\n",
        "4. Riesgos potenciales\n",
        "\n",
        "Formato: Encabezado, secciones claras, puntos clave destacados.\n",
        "\"\"\"\n",
        "\n",
        "# Llamada a la API\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_reporte}],\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Mostrar resultado\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203240d4",
      "metadata": {
        "id": "203240d4"
      },
      "outputs": [],
      "source": [
        "# Para convertirlo a Word:\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34ea678",
      "metadata": {
        "id": "a34ea678"
      },
      "outputs": [],
      "source": [
        "\n",
        "from docx import Document\n",
        "\n",
        "# Texto generado por el modelo\n",
        "texto = response.choices[0].message.content\n",
        "\n",
        "# Crear documento\n",
        "doc = Document()\n",
        "doc.add_heading(\"Reporte Ejecutivo\", level=1)\n",
        "\n",
        "for linea in texto.split(\"\\n\"):\n",
        "    doc.add_paragraph(linea)\n",
        "\n",
        "# Guardar como archivo Word\n",
        "doc.save(\"reporte_subestacion.docx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f65848f",
      "metadata": {
        "id": "8f65848f"
      },
      "outputs": [],
      "source": [
        "# Para convertirlo a PDF:\n",
        "\n",
        "!pip install reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e140f44",
      "metadata": {
        "id": "7e140f44"
      },
      "outputs": [],
      "source": [
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "texto = response.choices[0].message.content\n",
        "\n",
        "c = canvas.Canvas(\"reporte_subestacion.pdf\")\n",
        "c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "y = 800\n",
        "for linea in texto.split(\"\\n\"):\n",
        "    c.drawString(40, y, linea)\n",
        "    y -= 15\n",
        "    if y < 50:\n",
        "        c.showPage()\n",
        "        y = 800\n",
        "\n",
        "c.save()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73512bf5",
      "metadata": {
        "id": "73512bf5"
      },
      "source": [
        "### 2. Análisis de Datos con Prompts elaborados\n",
        "```python\n",
        "datos_consumo = \"\"\"\n",
        "Enero: 1250 MWh, 1.2% pérdidas\n",
        "Febrero: 1380 MWh, 1.5% pérdidas\n",
        "Marzo: 1420 MWh, 1.8% pérdidas\n",
        "Abril: 1560 MWh, 2.1% pérdidas\n",
        "\"\"\"\n",
        "\n",
        "prompt_analisis = f\"\"\"\n",
        "Analiza los siguientes datos de consumo y pérdidas:\n",
        "\n",
        "{datos_consumo}\n",
        "\n",
        "Realiza:\n",
        "1. Cálculo del incremento porcentual mensual en consumo\n",
        "2. Correlación entre consumo y pérdidas\n",
        "3. Identificación de patrones anómalos\n",
        "4. Recomendaciones técnicas\n",
        "\n",
        "Muestra tu razonamiento paso a paso antes de dar conclusiones.\n",
        "Usa formato Markdown con tablas para los cálculos.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_analisis}],\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fee3620",
      "metadata": {
        "id": "8fee3620"
      },
      "outputs": [],
      "source": [
        "datos_consumo = \"\"\"\n",
        "Enero: 1250 MWh, 1.2% pérdidas\n",
        "Febrero: 1380 MWh, 1.5% pérdidas\n",
        "Marzo: 1420 MWh, 1.8% pérdidas\n",
        "Abril: 1560 MWh, 2.1% pérdidas\n",
        "\"\"\"\n",
        "\n",
        "prompt_analisis = f\"\"\"\n",
        "Analiza los siguientes datos de consumo y pérdidas:\n",
        "\n",
        "{datos_consumo}\n",
        "\n",
        "Realiza:\n",
        "1. Cálculo del incremento porcentual mensual en consumo\n",
        "2. Correlación entre consumo y pérdidas\n",
        "3. Identificación de patrones anómalos\n",
        "4. Recomendaciones técnicas\n",
        "\n",
        "Muestra tu razonamiento paso a paso antes de dar conclusiones.\n",
        "Usa formato Markdown con tablas para los cálculos.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt_analisis}],\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "471d99c0",
      "metadata": {
        "id": "471d99c0"
      },
      "source": [
        "## Posibilidades / Ejercicios\n",
        "### Ejercicio 1: Asistente para Normativas Eléctricas\n",
        "1. Crea un asistente que responda preguntas sobre la norma IEEE 1547\n",
        "2. Implementa memoria de conversación (3-5 mensajes de historial)\n",
        "3. Añade capacidad de citar secciones específicas de la norma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62335b53",
      "metadata": {
        "id": "62335b53"
      },
      "source": [
        "### Ejercicio 2: Traductor Técnico\n",
        "1. Desarrolla un sistema que traduzca informes técnicos inglés-español\n",
        "2. Manteniendo terminología eléctrica precisa\n",
        "3. Con opción de explicar conceptos complejos al final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cd97a8",
      "metadata": {
        "id": "94cd97a8"
      },
      "source": [
        "### Ejercicio 3: Generador de Procedimientos\n",
        "1. Crea una función que genere procedimientos de mantenimiento\n",
        "2. Basado en tipo de equipo (transformador, interruptor, etc.)\n",
        "3. Y nivel de experiencia del técnico (junior, senior)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a893a9",
      "metadata": {
        "id": "f9a893a9"
      },
      "source": [
        "### Solución Ejercicio 1: Asistente para Normativas\n",
        "```python\n",
        "from collections import deque\n",
        "\n",
        "class AsistenteNormativas:\n",
        "    def __init__(self, client):\n",
        "        self.client = client  # Usar cliente global ya configurado\n",
        "        self.historial = deque(maxlen=5)\n",
        "        self.contexto = \"\"\"\n",
        "        Eres un experto en la norma IEEE 1547 para interconexión de recursos distribuidos.\n",
        "        Responde preguntas técnicas citando los artículos relevantes.\n",
        "        Sé preciso y usa lenguaje profesional.\n",
        "        \"\"\"\n",
        "\n",
        "    def preguntar(self, consulta):\n",
        "        self.historial.append({\"role\": \"user\", \"content\": consulta})\n",
        "        \n",
        "        messages = [{\"role\": \"system\", \"content\": self.contexto}] + list(self.historial)\n",
        "        \n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        \n",
        "        respuesta = response.choices[0].message.content\n",
        "        self.historial.append({\"role\": \"assistant\", \"content\": respuesta})\n",
        "        return respuesta\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3bc1f7",
      "metadata": {
        "id": "ad3bc1f7"
      },
      "source": [
        "# Se usa:\n",
        "asistente = AsistenteNormativas(client)\n",
        "asistente.preguntar(\"¿Qué exige la norma IEEE 1547 en cuanto a frecuencia?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccec9781",
      "metadata": {
        "id": "ccec9781"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class AsistenteNormativas:\n",
        "    def __init__(self, client):\n",
        "        self.client = client  # Usar cliente global ya configurado\n",
        "        self.historial = deque(maxlen=5)\n",
        "        self.contexto = \"\"\"\n",
        "        Eres un experto en la norma IEEE 1547 para interconexión de recursos distribuidos.\n",
        "        Responde preguntas técnicas citando los artículos relevantes.\n",
        "        Sé preciso y usa lenguaje profesional.\n",
        "        \"\"\"\n",
        "\n",
        "    def preguntar(self, consulta):\n",
        "        self.historial.append({\"role\": \"user\", \"content\": consulta})\n",
        "\n",
        "        messages = [{\"role\": \"system\", \"content\": self.contexto}] + list(self.historial)\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        respuesta = response.choices[0].message.content\n",
        "        self.historial.append({\"role\": \"assistant\", \"content\": respuesta})\n",
        "        return respuesta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "314f9f6d",
      "metadata": {
        "id": "314f9f6d"
      },
      "outputs": [],
      "source": [
        "asistente = AsistenteNormativas(client)\n",
        "asistente.preguntar(\"¿Qué exige la norma IEEE 1547 en cuanto a frecuencia?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3419640f",
      "metadata": {
        "id": "3419640f"
      },
      "source": [
        "## Buenas Prácticas para APIs de LLMs\n",
        "\n",
        "### 1. Manejo de Errores\n",
        "```python\n",
        "try:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        timeout=10  # Timeout en segundos\n",
        "    )\n",
        "except openai.error.APIError as e:\n",
        "    print(f\"Error de API: {e}\")\n",
        "except openai.error.Timeout as e:\n",
        "    print(f\"Timeout: {e}\")\n",
        "except openai.error.RateLimitError as e:\n",
        "    print(f\"Límite de tasa excedido: {e}\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f37bced",
      "metadata": {
        "id": "8f37bced"
      },
      "source": [
        "### 3. Segmentación de Respuestas Largas\n",
        "```python\n",
        "def get_long_response(prompt, max_tokens=4000):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=max_tokens,\n",
        "        stream=True  # Stream para respuestas largas\n",
        "    )\n",
        "    \n",
        "    full_response = []\n",
        "    for chunk in response:\n",
        "        content = chunk.choices[0].delta.get(\"content\", \"\")\n",
        "        print(content, end=\"\", flush=True)\n",
        "        full_response.append(content)\n",
        "    \n",
        "    return \"\".join(full_response)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4852a33",
      "metadata": {
        "id": "e4852a33"
      },
      "source": [
        "## Proyecto-: Sistema de Asistencia Técnica Integrado\n",
        "\n",
        "**Objetivo:** Crear un sistema que:\n",
        "1. Analice datos SCADA en tiempo real\n",
        "2. Genere alertas inteligentes\n",
        "3. Proporcione recomendaciones accionables\n",
        "4. Documente incidentes automáticamente\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cad3515",
      "metadata": {
        "id": "8cad3515"
      },
      "source": [
        "\n",
        "```python\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "class AsistenteSCADA:\n",
        "    def __init__(self, client):\n",
        "        self.client = client  # Cliente ya configurado\n",
        "        self.contexto = \"\"\"\n",
        "        Eres un ingeniero de sistemas de potencia experimentado.\n",
        "        Analiza datos de SCADA y proporciona recomendaciones técnicas.\n",
        "        \"\"\"\n",
        "\n",
        "    def analizar_datos(self, datos):\n",
        "        prompt = f\"\"\"\n",
        "        Analiza estos datos de SCADA:\n",
        "        {datos.to_markdown(index=False)}\n",
        "\n",
        "        Identifica:\n",
        "        1. Anomalías o valores fuera de rango\n",
        "        2. Posibles causas\n",
        "        3. Acciones recomendadas\n",
        "        4. Prioridad (Alta, Media, Baja)\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": self.contexto},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e837289",
      "metadata": {
        "id": "0e837289"
      },
      "source": [
        "Se ejecuta:\n",
        "\n",
        "datos = pd.DataFrame({\n",
        "    \"Parámetro\": [\"Voltaje L1\", \"Voltaje L2\", \"Voltaje L3\", \"Temp. Transformador\"],\n",
        "    \"Valor\": [132.4, 131.8, 85.2, 78],\n",
        "    \"Límite\": [\"130±5%\", \"130±5%\", \"130±5%\", \"75 max\"]\n",
        "})\n",
        "\n",
        "asistente = AsistenteSCADA(client)\n",
        "print(asistente.analizar_datos(datos))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf226a1",
      "metadata": {
        "id": "baf226a1"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "class AsistenteSCADA:\n",
        "    def __init__(self, client):\n",
        "        self.client = client  # Cliente ya configurado\n",
        "        self.contexto = \"\"\"\n",
        "        Eres un ingeniero de sistemas de potencia experimentado.\n",
        "        Analiza datos de SCADA y proporciona recomendaciones técnicas.\n",
        "        \"\"\"\n",
        "\n",
        "    def analizar_datos(self, datos):\n",
        "        prompt = f\"\"\"\n",
        "        Analiza estos datos de SCADA:\n",
        "        {datos.to_markdown(index=False)}\n",
        "\n",
        "        Identifica:\n",
        "        1. Anomalías o valores fuera de rango\n",
        "        2. Posibles causas\n",
        "        3. Acciones recomendadas\n",
        "        4. Prioridad (Alta, Media, Baja)\n",
        "        \"\"\"\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": self.contexto},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4aaba0b",
      "metadata": {
        "id": "f4aaba0b"
      },
      "outputs": [],
      "source": [
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787f5ce7",
      "metadata": {
        "id": "787f5ce7"
      },
      "outputs": [],
      "source": [
        "datos = pd.DataFrame({\n",
        "    \"Parámetro\": [\"Voltaje L1\", \"Voltaje L2\", \"Voltaje L3\", \"Temp. Transformador\"],\n",
        "    \"Valor\": [132.4, 131.8, 85.2, 78],\n",
        "    \"Límite\": [\"130±5%\", \"130±5%\", \"130±5%\", \"75 max\"]\n",
        "})\n",
        "\n",
        "asistente = AsistenteSCADA(client)\n",
        "print(asistente.analizar_datos(datos))"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}